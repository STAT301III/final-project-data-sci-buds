---
title: "Exploratory Data Analysis"
author: "Data Sci Buds (Pranav, Lily, Minjee, Tammy)"
date: '2022-04-27'
output:
  html_document:
    toc: true
    toc_float: true
    highlight: "tango"
    code_folding: "hide"
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE, warning = F}
#load packages 
library(tidyverse)
library(tidymodels)
library(janitor)
library(naniar)
library(ggplot2)
```

# Read in Data & Intro 

```{r, warning = F, message = F}
#read in 
superstore_dat <- read_csv(file = "data/unprocessed/Superstore_data.csv") %>% 
  clean_names()
```

<br>

The dataset involves purchase information for different products sold in a large superstore. The dataset was collected by the company itself. Each row is a customer transaction, and the columns represent different information for that transaction (like product name, quantity sold, profit, etc.). The data is from kaggle. A citation is attached below. 

<br>

https://www.kaggle.com/datasets/vivek468/superstore-dataset-final

<br>


# Initial Split of Data

```{r, warning = F, message = F}
# initial split 
#split data
superstore_split <- 
  initial_split(superstore_dat,
                prop = .8,
                strata = profit)

superstore_train <- training(superstore_split)
superstore_test <- testing(superstore_split)
```

<br>

First we split the data into training and test set. We want to conduct the EDA on the training data. Ideally, we could subset a portion of the data just for EDA, but we prefer to conserve some data. 

<br>

# Initial Overview of Data

```{r, message = F, warning = F}
#9994 observations and 21 features 
dim(superstore_dat)
```

<br>

We see that there are 9994 observations and 21 features in the data.

<br>

```{r, message = F, warning = F}
#check for missingness 
miss_var_table(superstore_train)
miss_var_summary(superstore_train)
gg_miss_var(superstore_train)
```

<br>

As we see above, there is no missingness in the data. Hence, there will be nothing for us to address in this respect. We considered adding on another dataset to gain more information about income for a customer's zip code, but some of these datasets had a great deal of missingess or are just inaccessible. We have yet to find a good income per zip code dataset that we could potentially merge with our current dataset.  

<br>

# Essential Findings

## Response Variables

```{r, message = F, warning = F}
# univariate profit graph
ggplot(superstore_dat, aes(profit)) +
  geom_density() +
  xlim(c(-100, 150)) +
  ggtitle("Profit or Loss Incurred from Sales")
```

<br>

We can see that profit peaks at around $10-15 per product transaction.  

<br>

```{r, warning = F, message = F}
# univariate sales graph
ggplot(superstore_dat, aes(sales)) +
  geom_density() +
  xlim(c(0,500)) +
  ggtitle("Sales of Products")
```

<br>

Here we can see that sales (revenue) peaks at around $30 per product transaction. There is a clear right skew, which is what we would expect. There are some transactions that have exhibit a high sales value, probably for expensive items, but these are relatively uncommon in occurrence.

<br>

## Important Predictor Variables

```{r, warning = F, message = F}
# univariate category graph
ggplot(superstore_dat, aes(category)) +
  geom_bar() +
  ggtitle("Category of Products Sold")
```

<br>
Out of the products sold, office supplies were sold the most and technology was sold the least.
<br><br>

```{r, warning = F, message = F}
# univariate subcategory graph
ggplot(superstore_dat, aes(sub_category)) +
  geom_bar() +
  ggtitle("Subcategory of Products Sold") +
  coord_flip()
```

<br>
Out of the subcategories of products sold, binders were sold the most and copiers were sold the least. The top 2 subcategories (binders and paper) are both under office supplies so it makes sense that office supplies are the most sold category of products. 
<br><br>

```{r, warning = F, message = F}
#univariate region graph
ggplot(superstore_dat, aes(region)) +
  geom_bar() +
  ggtitle("Region where Customer is From")
```

<br>
The most number of customers are from the Western area of the US while the least number of customers are from the Southern area of the US.
<br><br>

```{r, warning = F, message = F}
# univariate discount graph
ggplot(superstore_dat, aes(discount)) +
  geom_density() +
  ggtitle("Discount Provided")
```

<br>
For discounts provided, the largest discount provided is 0%, but there is also another peak at 20%, indicating that there might be certain sales throughout the year that are specifically 20% off discounts. 
<br><br>

```{r, warning = F, message = F}
# univariate state graph
ggplot(superstore_dat, aes(state)) +
  geom_bar() +
  ggtitle("Customer State of Residence") +
  coord_flip()
```

<br>
The majority of customers are from California, which correlates with the largest number of customers coming from the Western area of the US. 
<br><br>

```{r, warning = F, message = F}
# bivariate sub-category graph
superstore_dat %>%
  group_by(sub_category) %>%
  mutate(avg_prf = mean(profit)) %>%
  ggplot(aes(sub_category, avg_prf)) +
  geom_bar(stat="identity", position=position_dodge()) +
  labs(
    title = "Average Profit for Different Sub-Categories",
    x = "Subcategory",
    y = "Average Profit"
  ) +
  theme(axis.text.x = element_text(angle = 90))
```
<br>
From the graph above we can see that the copier subcategory brings exceptionally more profit than the other subcategories, with an average around 800. This is a very interesting relationship between response and predictor variable. From this, I think that the subcategory of profit is going to be very important when predicting profit, especially because of the huge profits brought by copiers! I wonder why they probably upcharge on copiers so much. Maybe that is just the market rate?
<br><br>

```{r, warning = F, message = F}
# same plot not including copiers  
superstore_dat %>%
  group_by(sub_category) %>%
  mutate(avg_prf = mean(profit)) %>%
  ggplot(aes(sub_category, avg_prf)) +
  geom_bar(stat="identity", position=position_dodge()) +
  scale_y_continuous(limits = c(-60, 60),
                     breaks = seq(-60, 60, 20)) +
  labs(
    title = "Average Profit for Different Sub-Categories",
    subtitle = "(Not including Copiers sub-category)",
    x = "Subcategory",
    y = "Average Profit"
  ) +
  theme(axis.text.x = element_text(angle = 90))
```
<br>
Omitting the copiers for a closer look on the other subcategories, we can see that they average around 30 in mean profit. In addition, the bookcases, supplies and tables subcategories are negative in profit, with the tables subcategory having the greatest loss of around 55. 
<br><br>

```{r, warning = F, message = F}
# bivariate discount graph - profit
superstore_dat %>%
  group_by(discount) %>%
  mutate(avg_prf = mean(profit)) %>%
  ggplot(aes(discount, avg_prf)) +
  geom_bar(stat="identity", position=position_dodge(), width = 0.04) +
  scale_x_continuous(limits = c(-0.05, 0.85),
                     breaks = seq(-0.1, 0.8, 0.1),
                     expand = c(0,0)) +
  labs(
    title = "Average Profit for Different Discount Values",
    x = "Discount Value",
    y = "Average Profit"
  )
```
<br>
From the relationship between discount and profit, we can see that discount values above 0.3 generally bring a loss as opposed to a profit, with losses peaking at a discount value of 0.5. The discount value that brings the most average profit is 0.1.
<br><br>

```{r, warning = F, message = F}
# bivariate discount graph - sales
superstore_dat %>%
  group_by(discount) %>%
  mutate(avg_sls = mean(sales)) %>%
  ggplot(aes(discount, avg_sls)) +
  geom_bar(stat="identity", position=position_dodge(), width = 0.04) +
  scale_x_continuous(limits = c(-0.05, 0.85),
                     breaks = seq(-0.1, 0.8, 0.1),
                     expand = c(0,0)) +
  labs(
    title = "Average Sales for Different Discount Values",
    x = "Discount Value",
    y = "Average Sales"
  )
```
<br>
Contrastingly, average sales are reflected differently with discounts, with a 0.5 discount bringing the highest average sales of over 875, and the 0.1 discount coming second highest in sales. Depending on whether the supermarket chooses to maximize profit or revenue, they can choose discount values of 0.1 for the former and 0.5 for the latter. 
<br><br>

```{r, warning = F, message = F}
# bivariate table - top 10 states in sales
superstore_dat %>%
  group_by(state) %>%
  mutate(average_sales = mean(sales)) %>%
  select(state, average_sales) %>%
  arrange(desc(average_sales)) %>%
  distinct() %>%
  head(10)

# bivariate table - top 10 states in profit 
superstore_dat %>%
  group_by(state) %>%
  mutate(average_profit = mean(profit)) %>%
  select(state, average_profit) %>%
  arrange(desc(average_profit)) %>%
  distinct() %>%
  head(10)
```
<br>
Shown above are two tables showing respectively the top 10 states with the highest average sales and average profit. The two tables have a high overlap of 7 out of 10 states: Wyoming, Vermont, Rhode Island, Montana, Indiana, Missouri, and Minnesota. In particular, Vermont is highest in profit and second highest in sales.
<br><br>

```{r, warning = F, message = F}
# region sales contribution and segment composition
superstore_dat %>%
  group_by(region) %>%
  mutate(avg_sls = mean(sales)) %>%
  ggplot(aes(x = region, y = avg_sls, fill = segment)) +
  geom_bar(stat="identity") +
  scale_y_continuous(limits = c(0, 750000),
                     breaks = seq(0, 800000, 100000),
                     expand = c(0,0),
                     labels = comma) +
  labs(
    title = "Region Sales and Segment Composition",
    x = "Region",
    y = "Average Sales",
    fill = "Customer Segment"
  ) + 
  theme_light()

```
<br>
This stacked bar graph shows the average sales contribution and customer segment composition of each region. This  shows many interesting findings between predictor variables. First, we can see that in general, the consumer segment brings a larger sales contribution as opposed to corporate and home office segment. Second, we can see that the West region is highest in average sales. Third, it looks like for the Home Office segment, West and East have the highest sales. Maybe a lot of people work from home here? 
<br><br>

```{r, warning = F, message = F}
# west region state sales and segment
superstore_dat %>%
  filter(region == "West") %>%
  group_by(state) %>% 
  mutate(avg_sls = mean(sales)) %>%
  ggplot(aes(x = state, y = avg_sls, fill = segment)) +
  geom_bar(stat="identity") +
  scale_y_continuous(limits = c(0, 475000),
                     breaks = seq(0, 500000, 50000),
                     expand = c(0,0),
                     labels = comma) +
  labs(
    title = "Western States Sales and Segment Composition",
    x = "State",
    y = "Average Sales",
    fill = "Customer Segment"
  ) + 
  theme_light() +
  theme(axis.text.x = element_text(angle = 90))
```
<br>
This stacked bar graph in turn shows the average sales and customer segment composition of the Western states, where California seems to bring the highest sales contribution, with Washington as second highest. 
<br><br>

# Secondary Findings

## Standard Variable Explorations

```{r, warning = F, message = F}
#univariate quantity graph
ggplot(superstore_dat, aes(quantity)) +
  geom_density() +
  ggtitle("Quantity of the Product Ordered") 
```

<br>
In this graph, there are 2 large peaks, indicating that there are different quantities where customers bought the most. The quantities are 2 and 3 so a lot of people bought 2 or 3 products from this store. This is unsurprising, because we could expect that most transactions involve a low quantity of a product bought. Most people wouldn't have use for more than one of a specific productâ€”also, it would be more expensive to buy more. 
<br><br>

```{r, warning = F, message = F}
# univariate ship mode graph
ggplot(superstore_dat, aes(ship_mode)) +
  geom_bar() +
  ggtitle("Order Ship Mode")  
```

<br>
The majority of people used standard class shipping while the least number of people used same day shipping. This makes sense since standard class shipping is the cheapest while same day shipping is the most expensive. However, this could still be a very useful variable to employ. It is possible that, for examples, copiers (which give a super high profit on average as previously shown) are shipped commonly in one specific way. Maybe since they are often bought by business, business don't mind paying extra and getting it quicker. Hence, this could prove to be a very important variable. 
<br><br>

Overall, this EDA is important because we want to know how to transform variables via feature engineering to make our model as predictive as possible. For example, if some variables have a heavy skew, we could consider doing a log transformation. Also, it might be useful to see if there is a super high correlation between two predictor variables. If there is, then we might want to avoid some model types that are sensitive to this issue (like multiple linear regression). We could consider some principal component analysis if it makes sense. It was interesting to see different relationships between our variables in the first place. For example, it is interesting to see which segments of sales are common in different areas in the country. This might be due to different consumer preferences and could certainly be useful for the store when deciding which products to ship to different US stores.

# Github  Repo 
[https://github.com/STAT301III/final-project-data-sci-buds.git](https://github.com/STAT301III/final-project-data-sci-buds.git){target="_blank"}








