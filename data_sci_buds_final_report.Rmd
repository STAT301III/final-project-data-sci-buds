---
title: "Stat 301-3 Final Report"
author: "Data Sci Buds (Pranav, Lily, Minjee, Tammy)"
date: '2022-06-07'
output:
  html_document:
    toc: true
    toc_float: true
    highlight: "tango"
    code_folding: "hide"
editor_options: 
  chunk_output_type: console
---

## Data and Research Question
The dataset involves purchase information for different products sold in a large superstore. The dataset was collected by the company itself. Each row is a customer transaction, and the columns represent different information for that transaction (like product name, quantity sold, profit, etc.). Our research question is: what variables affect how much profit the company earns on a specific transaction? 
<br>
**Citation:** https://www.kaggle.com/datasets/vivek468/superstore-dataset-final

```{r, message = F, warning = F, error = F}
#load packages 
library(tidyverse)
library(tidymodels)
library(janitor)
library(naniar)
library(ggplot2)

# load data
superstore_dat <- read_csv(file = "data/unprocessed/Superstore_data.csv") %>% 
  clean_names() %>% 
  mutate(
    ship_mode = factor(ship_mode),
    segment = factor(segment),
    city = factor(city),
    state = factor(state),
    region = factor(region),
    category = factor(category),
    sub_category = factor(sub_category)
  )
```

## EDA

From the graph below, we can see that profit peaks at around $10-15 per product transaction.
```{r, message = F, warning = F}
# univariate profit graph
ggplot(superstore_dat, aes(profit)) +
  geom_density() +
  xlim(c(-100, 150)) +
  ggtitle("Profit or Loss Incurred from Sales")
```

From the graph below we can see that the copier subcategory brings exceptionally more profit than the other subcategories, with an average around 800. This is a very interesting relationship between response and predictor variable. From this, we believe that the subcategory of profit is going to be very important when predicting profit, especially because of the huge profits brought by copiers.

```{r, message = F, warning = F}
# bivariate sub-category graph
superstore_dat %>%
  group_by(sub_category) %>%
  mutate(avg_prf = mean(profit)) %>%
  ggplot(aes(sub_category, avg_prf)) +
  geom_bar(stat="identity", position=position_dodge()) +
  labs(
    title = "Average Profit for Different Sub-Categories",
    x = "Subcategory",
    y = "Average Profit"
  ) +
  theme(axis.text.x = element_text(angle = 90))
```

From the relationship between discount and profit, we can see that discount values above 0.3 generally bring a loss as opposed to a profit, with losses peaking at a discount value of 0.5. The discount value that brings the most average profit is 0.1.

```{r, message = F, warning = F}
# bivariate discount graph - profit
superstore_dat %>%
  group_by(discount) %>%
  mutate(avg_prf = mean(profit)) %>%
  ggplot(aes(discount, avg_prf)) +
  geom_bar(stat="identity", position=position_dodge(), width = 0.04) +
  scale_x_continuous(limits = c(-0.05, 0.85),
                     breaks = seq(-0.1, 0.8, 0.1),
                     expand = c(0,0)) +
  labs(
    title = "Average Profit for Different Discount Values",
    x = "Discount Value",
    y = "Average Profit"
  )
```

From the table below, we can see that Vermont is highest in profit and second highest in sales, followed by Rhode Island, Indiana, Montana, and Minnesota. 

```{r, message = F, warning = F}
# bivariate table - top 10 states in profit 
superstore_dat %>%
  group_by(state) %>%
  mutate(average_profit = mean(profit)) %>%
  select(state, average_profit) %>%
  arrange(desc(average_profit)) %>%
  distinct() %>%
  head(10)
```


## Model Fitting

First, we split the data into a training and testing set, splitting it by using 80% as the proportion of data that is in the training set and 20% in the testing set. The training set will be used to create the models, while the testing set will be used to fit the final models. We stratified the data by the target variable, profit, to counter for class imbalances in the data. Next, we performed a v-fold cross validation and folded the training data, using 5 folds and 3 repeats. Since resampling is only performed on the training set, it helps us understand the effectiveness of the model before using the test set. 

```{r, message = F, error = F, warning = F}
# data splitting
superstore_split <- 
  initial_split(superstore_dat,
                prop = .8,
                strata = profit)

superstore_train <- training(superstore_split)
superstore_test <- testing(superstore_split)

# set folds
superstore_resamples <- 
  superstore_train %>% 
  vfold_cv(v = 5, repeats = 3, strata = profit)
```

For the recipe, we used `step_rm` to remove certain predictors that we knew would not help predict profit at all, such as `row_id`, `customer_id`, `customer_name`, and other shipping information. We used a couple `step_` functions for this recipe - `step_other`, which pools infrequently occurring values into an "other" category, `step_dummy`, which converts the nominal data into numeric binary model terms, `step_normalize`, which normalizes all the numeric predictors to have a standard deviation of 1 and a mean of 0, and finally, we used `step_nvz` on all predictors, so that variables that are highly unbalanced and sparse would be removed.  

```{r, message = F, error = F, warning = F}
# set up recipe
superstore_rec <- recipe(profit ~ ., data = superstore_dat) %>%
  step_rm(row_id, order_id, order_date, ship_date, customer_id, customer_name,
          country, postal_code, product_id, product_name) %>%
  step_other(all_nominal_predictors()) %>%  
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_nzv(all_predictors())

# load recipe
load(file = "model_info/model_setup.rda")
```


### Elastic Net

We trained and tuned an elastic net model, setting up a workflow, and adding the recipe above. We tuned the penalty and mixture of the model. 
```{r, eval = F}
# define model
enet_spec <- 
  linear_reg(mode = "regression",
             penalty = tune(), 
             mixture = tune()) %>% 
  set_engine("glmnet")

# set up/define tuning grid
enet_params <- parameters(enet_spec)
enet_grid <- grid_regular(enet_params, levels = 3)

# workflow
enet_workflow <- workflow() %>% 
  add_model(enet_spec) %>% 
  add_recipe(superstore_rec)
```

We can see that the best elastic net model has a penalty of 1 and a mixture of 0.05, which yielded an rmse of 181. 
```{r}
load(file = "results/tuned_models.rda")

show_best(enet_tuned, metric = "rmse") %>% head(1)
```

### K Nearest Neighbor

We trained and tuned a k nearest neighbor model, setting up a workflow, and adding the recipe above. We tuned the number of neighbors in this model.
```{r, eval = F}
# define model
knn_spec <- nearest_neighbor(mode = "regression",
                             neighbors = tune()) %>%
  set_engine("kknn") 

# set up/define tuning grid
knn_params <- parameters(knn_spec)
knn_grid <- grid_regular(knn_params, levels = 3)

# workflow
knn_workflow <- workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(superstore_rec)
```

The best k nearest neighbors model had 8 neighbors, which yielded an rmse of 170, which is better than the elastic net model above.
```{r}
show_best(knn_tuned, metric = "rmse") %>% head(1)
```

### Random Forest

We trained and tuned a random forest model, setting up a workflow, and adding the recipe above. For the limits for mtry, we set 1 as the lower limit and 20 as the upper limit, and we tuned min_n as well.
```{r, eval = F}
# define model
rf_spec <- rand_forest(mode = "regression",
                       min_n = tune(),
                       mtry = tune()) %>% 
  set_engine("ranger")

# set up/define tuning grid
rf_params <- parameters(rf_spec) %>% 
  update(mtry = mtry(c(1, 20))) 

rf_grid <- grid_regular(rf_params, levels = 3)

# workflow
rf_workflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(superstore_rec)
```

The best random forest model has an mtry of 20 and a min_n of 2. The rmse is 110, which is better than both the elastic net and k nearest neighbors models above.
```{r}
show_best(rf_tuned, metric = "rmse") %>% head(1)
```

### Boosted Tree

We trained and tuned a boosted tree model, setting up a workflow, and adding the recipe above. For the limits for mtry, we set 1 as the lower limit and 20 as the upper limit. We also tuned learn_rate and min_n.
```{r, eval = F}
# define model
boost_spec <- boost_tree(mode = "regression",
                         mtry = tune(),
                         min_n = tune(),
                         learn_rate = tune()) %>%
  set_engine("xgboost")

# set up/define tuning grid
boost_params <- parameters(boost_spec) %>%
  update(mtry = mtry(c(1, 20)),
         learn_rate = learn_rate(c(-5, -0.2)))

boost_grid <- grid_regular(boost_params, levels = 3)

# workflow
boost_workflow <- workflow() %>% 
  add_model(boost_spec) %>% 
  add_recipe(superstore_rec)
```

The best boosted tree model has an mtry of 20, min_n of 2, and a learn rate of 0.631. The rmse is 112, which is slightly worse than the random forest model above. 
```{r}
show_best(boost_tuned, metric = "rmse") %>% head(1)
```

### Support Vector Machine Polynomial

We trained and tuned a support vector machine polynomial model, setting up a workflow, and adding the recipe above. We tuned cost, degree, and scale_factor.
```{r, eval = F}
# define model
svm_p_spec <- svm_poly(mode = "regression",
                       cost = tune(), 
                       degree = tune(),
                       scale_factor = tune()) %>% 
  set_engine("kernlab") 

# set up/define tuning grid
svm_p_params <- parameters(svm_p_spec)
svm_p_grid <- grid_regular(svm_p_params, levels = 3)

# workflow
svm_r_workflow <- workflow() %>% 
  add_model(svm_r_spec) %>% 
  add_recipe(superstore_rec)
```

The best support vector machine polynomial model has a cost of 32, degree of 2, an a scale factor of 0.1, which yielded an rmse of 94.9, which is better than all of the previous models we have tested.
```{r}
show_best(svm_p_tuned, metric = "rmse") %>% head(1)
```

### Support Vector Machine Radial Base Function

We trained and tuned a support vector machine radial base function model, setting up a workflow, and adding the recipe above. We tuned cost and rbf_sigma.
```{r, eval = F}
# define model
svm_r_spec <- svm_rbf(mode = "regression",
                      cost = tune(), 
                      rbf_sigma = tune()) %>% 
  set_engine("kernlab")

# set up/define tuning grid
svm_r_params <- parameters(svm_r_spec)
svm_r_grid <- grid_regular(svm_r_params, levels = 3)

# workflow
svm_p_workflow <- workflow() %>% 
  add_model(svm_p_spec) %>% 
  add_recipe(superstore_rec)
```

The best support vector machine radial basis function model has a cost of 32 and an rbf_sigma of 0.00001, which yielded an rmse of 190, meaning that it is one of our worse models because of its high rmse. 
```{r}
show_best(svm_r_tuned, metric = "rmse") %>% head(1)
```

### Single Layer Neural Network

We trained and tuned a single layer neural network model, setting up a workflow, and adding the recipe above. We tuned hidden_units and penalty. 
```{r, eval = F}
# define model
nnet_spec <- mlp(mode = "regression",
                 hidden_units = tune(), 
                 penalty = tune()) %>% 
  set_engine("nnet")

# set up/define tuning grid
nnet_params <- parameters(nnet_spec)
nnet_grid <- grid_regular(nnet_params, levels = 3)

# workflow
nnet_workflow <- workflow() %>% 
  add_model(nnet_spec) %>% 
  add_recipe(superstore_rec)
```

The best single layer neural network model has 10 hidden_units and a penalty of 1, which yielded an rmse of 178, making it better than the svm_r model above but still worse than the boosted tree, svm_p, and random forest models. 
```{r}
show_best(nnet_tuned, metric = "rmse") %>% head(1)
```

### MARS
We trained and tuned a MARS model, setting up a workflow, and adding the recipe above. We tuned num_terms and prod_degree. 
```{r, eval = F}
# define model
mars_spec <- mars(mode = "regression",
                  num_terms = tune(),
                  prod_degree = tune()) %>%  
  set_engine("earth") 

# set up/define tuning grid
mars_params <- parameters(mars_spec)
mars_grid <- grid_regular(mars_params, levels = 3)

# workflow
mars_workflow <- workflow() %>% 
  add_model(mars_spec) %>% 
  add_recipe(superstore_rec)
```

The best MARS model has 5 num_terms and a prod_degree of 2, yielding an rmse of 155. 
```{r}
show_best(mars_tuned, metric = "rmse") %>% head(1)
```

## Best Model Fitting

We can see from all 8 models above that the best model is the support vector machine polynomial model, which yielded the lowest rmse of 94.9. The other models that  We then fit this winning model to the testing set, which ended up yielding a higher rmse of 167.
```{r}
load("results/best_model_fit_metric.rda")

svm_p_rmse_test
```

## Debrief and Next Steps

Overall, we tested a variety of 8 models: elastic net, k nearest neighbors, boosted tree, random forest, support vector machine polynomial, support vector machine radial, single layer neural network












